<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>MAmmoTH-VL</title>
    <link rel="icon" type="image/x-icon" href="static/images/mammoth_vl.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./data/results/data_setting.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>
  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
        <a class="navbar-item" href="https://opencodeinterpreter.github.io/">
            <b>OpenCodeInterpreter</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
            </a>
          <a class="navbar-item" href="https://mmmu-benchmark.github.io/">
            <b>MMMU</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
            <b>MAmmoTH</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH2/">
            <b>MAmmoTH2</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <!-- <h1 class="title is-1 publication-title">ü¶£ MAmmoTH-VL: <br>Scaling Synthetic Multimodal Instruction Data with Open Models</h1> -->
                        <h1 class="title is-1 publication-title">
                            <img src="static/images/mammoth_vl.png" alt="Mammoth Icon" style="vertical-align: -2px; width: 60px; height: 40px;"> MAmmoTH-VL: <br>Eliciting Multimodal Reasoning with Instruction Tuning at Scale
                          </h1>
                          
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                <span class="author-block">
                    <a href="" target="_blank">Jarvis Guo</a><sup>‚ô£*</sup>,</span>
                <span class="author-block">
                    <a href="" target="_blank">Tuney Zheng</a><sup>‚ô£*</sup>,</span>
                <span class="author-block">
                        <a href="" target="_blank">Yuelin Bai</a><sup>‚ô£</sup>,</span>
                <span class="author-block">
                        <a href="" target="_blank">Bo Li</a><sup>‚ñ≥</sup>
                </span>
                <br>
                <span class="author-block">
                    <a href="" target="_blank">Yubo Wang</a><sup>‚ô°</sup>,</span>
                <span class="author-block">
                    <a href="" target="_blank">King Zhu</a><sup>‚ô£</sup>,</span>
                <span class="author-block">
                    <a href="" target="_blank">Yizhi Li</a><sup>‚ô¢</sup>,
                </span>
                <br>
                <span class="author-block">
                    <a href="" target="_blank">Graham Neubig</a><sup>‚ô†</sup>,</span>
                <span class="author-block">
                    <a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a><sup>‚ô°</sup>,</span>
                <span class="author-block">
                    <a href="https://xiangyue9607.github.io/" target="_blank">Xiang Yue</a><sup>‚ô†‚Ä†</sup>,
                </span>
                        </div>



                <div class="is-size-5 publication-authors">
                    <span class="author-block">
                        <sup>‚ô†</sup>Carnegie Mellon University
                        <sup>‚ô£</sup>M-A-P
                        <br>
                        <sup>‚ñ≥</sup>Nanyang Technological University
                        <sup>‚ô°</sup>University of Waterloo
                        <br>
                        <sup>‚ô¢</sup>The University of Manchester
                        <br>
                    <!-- <span class="eql-cntrb"><small><sup>*</sup>Xiang Yue and Wenhu Chen are the leading authors of the project. They contributed equally to this project.</small></span> -->
                    <span class="author-block">‚Ä†Corresponding to: <a href="mailto:xyue2@andrew.cmu.edu">xyue2@andrew.cmu.edu</a>
                      <!-- , <a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a> </span> -->

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                          <a href="https://huggingface.co/datasets/MAmmoTH-VL/MAmmoTH-VL-Instruct-12M" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            ü§ó
                          </span>
                                <span>Dataset</span>
                                </a>
                                </span>

                                <!-- Supplementary PDF link -->
                                <span class="link-block">
                        <a href="https://huggingface.co/MAmmoTH-VL/MAmmoTH-VL-8B" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          ü§ó
                        </span>
                                <span>Model</span>
                                </a>
                                </span>
                                <span class="link-block">
                        <a href="https://huggingface.co/spaces/paralym/MAmmoTH-VL-8B" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                            ü§ó
                        </span>
                                <span>Demo</span>
                                </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/MAmmoTH-VL/MAmmoTH-VL" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="https://arxiv.org/pdf/2412.05237" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- Paper abstract -->
   <!-- Paper Abstract Section -->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                            <p>
                                Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a \textit{scalable and cost-effective method} to construct a \textit{large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning}. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.
                            <p>
    <!-- Image1 carousel -->
        
    <!-- <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-four-fifths"> -->
                        <div class="item">
                            <img src="static/images/mammoth_vl_overview.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                            <h2 class="subtitle" style="font-size: 16px; color: #888888;font-weight: normal;">
                                Figure 1: Up shows overview of our simple but scalable visual instruction data rewriting pipeline, comprising three main steps. First, we manually collect and identify potential data sources. Second, we rewrite the original instruction data using open MLLMs and LLMs. Finally, we employ the same MLLM as a judge to filter the data. Below shows examples comparing pre- and post-rewriting results in two categories: Math and Caption, demonstrating how the pipeline transforms basic questions into detailed, step-by-step responses.
                            </h2>
                        </div>
                    <!-- </div>
                </div>
            </div>
        </div>
    </section> -->
    
    <!-- End image1 carousel -->
    
    <p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End of Paper Abstract Section -->





<!--begining introduction-->

<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">
                        <p>
                            The rapid development of multimodal large language models (MLLMs) represents a significant advancement in the field of AI. Among the techniques driving this progress in the open-source community, the ‚Äúconnector-training‚Äù approaches, such as LLaVA, have gained prominence due to their simplicity and effectiveness. Connector training integrates a pretrained visual encoder, such as a Vision Transformer (ViT), with a pretrained large language model (LLM) via a lightweight adapter module or projection layer. This alignment enables the model to bridge textual and visual modalities, and fine-tuning on high-quality visual instruction data enhances multimodal capabilities for tasks ranging from image captioning to complex reasoning. However, a central challenge is the need for high-quality supervised fine-tuning (SFT) data, which is essential for adapting pretrained components to multimodal learning tasks.
                        </p>
                            
                        <p>
                            Generating such large-scale, high-quality multimodal SFT datasets poses significant challenges, particularly in open-source contexts with limited resources. Existing approaches typically rely on three methods: (1) human annotation, which is precise but expensive and labor-intensive, (2) repurposing existing academic datasets, such as visual question-answering datasets, which are accessible but lack diversity and in-depth reasoning content, and (3) querying proprietary models like GPT-4, which can produce high-quality data but come with high costs and licensing issues. To address this, we propose a simple, scalable, and cost-effective methodology for constructing a high-quality multimodal dataset, leveraging only open models. Our approach includes collecting and categorizing a diverse set of images from internet sources, followed by task-specific data augmentation and rewriting using open-weight LLMs and MLLMs, and concluding with rigorous quality filtering to remove irrelevant content and alleviate model bias.
                        </p>
                            
                        <p>
                            Our experimental results demonstrate the effectiveness of this approach. Using a curated dataset of 12 million visual instruction entries, we train an MLLM model based on the LLaVA-OneVision architecture. Evaluation across various downstream tasks reveals that our dataset significantly improves model performance, particularly in complex reasoning tasks, accuracy in visual question answering, and alignment between visual and textual content. By utilizing open models throughout the process, we are able to accelerate the development of multimodal large language models without relying on proprietary resources. The results highlight the potential of our method to advance the science of open MLLMs while maintaining accessibility and scalability.
                        </p>
                            
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <img src="static/images/mammoth_vl_method.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 2: 
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
 -->


<!--end begining introduction-->
<!--begining WEBINSTRUCT-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Data Generation Pipeline</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            While previous efforts have highlighted the potential of visual instruction tuning, many rely on resource-intensive methods such as human annotations or proprietary models. These approaches limit scalability and accessibility, particularly in open-source contexts. To address these challenges, we introduce a simple, scalable, and cost-effective data generation pipeline that produces 12 million high-quality samples. Our pipeline involves three key steps:    
                        </p>
                        <p>
                            (1) open-source data collection and categorization
                        </p>
                        <p>
                            (2) task-specific data augmentation and rewriting using open models
                        </p>
                        <p>
                            (3) quality filtering to remove hallucinated or irrelevant content
                        </p>
<!-- Image3 carousel -->
    
<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths"> -->
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/mammoth_vl_12M.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 2: The data distribution of MAmmoTH-VL-Instruct (12M). Left: Category distribution. Right: Details of data sources.
                        </h2>
                    </div>
                <!-- </div>
            </div>
        </div>
    </div>
</section> -->


<!--end begining introduction-->
<!--begining WEBINSTRUCT-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Distribution Comparison</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To analyze the distributional differences between the original and rewritten data, we randomly sampled 80,000 examples from the dataset both before and after rewriting and visualized their distributions using t-SNE to project the instructions onto a two-dimensional plot in Figure 3. The resulting figure reveals two key takeaways:
                        <p>
                            (1) The rewritten data exhibits significant overlap with the original data, indicating that it retains the core characteristics of the original distribution. This ensures that the rewritten data preserves the foundational structure of the dataset.
                        <p>
                            (2) The rewritten data extends beyond the boundaries of the original distribution, demonstrating that it introduces new dimensions or variations, which shows that rewriting enhances the dataset by broadening its scope while maintaining its original essence.
                        </p>
                        <p>
                            Based on this observation, during the experimental validation phase, we utilize a mixed dataset consisting of 70% rewritten data and 30% original data to train the model. 
                        </p>
<!-- Image3 carousel -->
    
<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths"> -->
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/mammoth_vl_distri.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; color: #888888;font-weight: normal;">
                            Figure 3: The t-SNE data distribution plot demonstrates how the rewritten data expands beyond the original dataset, increasing topic diversity and enhancing coverage of complex queries and reasoning. 
                        </h2>
                    </div>
                <!-- </div>
            </div>
        </div>
    </div>
</section> -->

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Overall Results</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To reveal the generality and effectiveness of the model, we comprehensively evaluate it across different scenarios, including single-image, multi-image, and video benchmarks. Detailed results are presented in Table 1, Table 2 and Table 3, respectively. We denote the model checkpoint that completed the single-image stage and one-vision stage as MAmmoTH-VL-8B (SI) and MAmmoTH-VL-8B. We conduct standardized, reproducible evaluations of our model across all 23 benchmarks using LMMs-Eval. To ensure a fair comparison with other MLLMs, we primarily report results from the original papers. When results are unavailable, we onboard the models in LMMs-Eval and evaluate them using consistent settings. All results are reported using greedy decoding and zero-shot settings unless specified.
                        </p>
<!-- Image3 carousel -->



<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="8" style="text-align: center;">Multi-Discipline Knowledge and Mathematical Reasoning</th>
        </tr>
        <tr>
            <th style="text-align: center;">MMStar<br>test</th>
            <th style="text-align: center;">MMMU<br>val</th>
            <th style="text-align: center;">MMMU-Pro<br>vision</th>
            <th style="text-align: center;">SeedBench<br>test</th>
            <th style="text-align: center;">MMBench<br>en-test</th>
            <th style="text-align: center;">MMVet<br>test</th>
            <th style="text-align: center;">MathVerse<br>mini-vision</th>
            <th style="text-align: center;">MathVista<br>testmini</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">69.1</td>
            <td style="text-align: center;">49.7</td>
            <td style="text-align: center;">76.2</td>
            <td style="text-align: center;">82.1</td>
            <td style="text-align: center;">76.2</td>
            <td style="text-align: center;">50.2</td>
            <td style="text-align: center;">63.8</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Gemini-1.5-Pro</td>
            <td style="text-align: center;">59.1</td>
            <td style="text-align: center;">65.8</td>
            <td style="text-align: center;">44.4</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">73.9</td>
            <td style="text-align: center;">64.0</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">63.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Claude-3.5-Sonnet</td>
            <td style="text-align: center;">62.2</td>
            <td style="text-align: center;">68.3</td>
            <td style="text-align: center;">48.0</td>
            <td style="text-align: center;">72.2</td>
            <td style="text-align: center;">79.7</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">67.7</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL2-LLaMa3-76B</td>
            <td style="text-align: center;">67.1</td>
            <td style="text-align: center;">58.2</td>
            <td style="text-align: center;">38.0</td>
            <td style="text-align: center;">77.6</td>
            <td style="text-align: center;">86.5</td>
            <td style="text-align: center;">64.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">65.5</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-72B-Ins</td>
            <td style="text-align: center;">68.6</td>
            <td style="text-align: center;">64.5</td>
            <td style="text-align: center;">37.1</td>
            <td style="text-align: center;">77.9</td>
            <td style="text-align: center;">86.9</td>
            <td style="text-align: center;">73.9</td>
            <td style="text-align: center;">37.3</td>
            <td style="text-align: center;">70.5</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B (SI)</td>
            <td style="text-align: center;">65.2</td>
            <td style="text-align: center;">57.4</td>
            <td style="text-align: center;">26.0</td>
            <td style="text-align: center;">77.6</td>
            <td style="text-align: center;">86.6</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">37.7</td>
            <td style="text-align: center;">66.5</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">66.1</td>
            <td style="text-align: center;">56.8</td>
            <td style="text-align: center;">24.0</td>
            <td style="text-align: center;">78.0</td>
            <td style="text-align: center;">85.9</td>
            <td style="text-align: center;">63.7</td>
            <td style="text-align: center;">39.1</td>
            <td style="text-align: center;">67.5</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>MiniCPM-V-2.6-7B</td>
            <td style="text-align: center;">57.5</td>
            <td style="text-align: center;">49.8</td>
            <td style="text-align: center;">21.7</td>
            <td style="text-align: center;">74.0</td>
            <td style="text-align: center;">81.5</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">60.6</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternLM-XComp-2.5-7B</td>
            <td style="text-align: center;">59.9</td>
            <td style="text-align: center;">42.9</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">74.4</td>
            <td style="text-align: center;">51.7</td>
            <td style="text-align: center;">20.0</td>
            <td style="text-align: center;">59.6</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Llama-3.2-11B-Vision-Ins</td>
            <td style="text-align: center;">49.8</td>
            <td style="text-align: center;">50.7</td>
            <td style="text-align: center;">23.7</td>
            <td style="text-align: center;">72.7</td>
            <td style="text-align: center;">73.2</td>
            <td style="text-align: center;">57.6</td>
            <td style="text-align: center;">23.6</td>
            <td style="text-align: center;">51.5</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">49.3</td>
            <td style="text-align: center;">25.4</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">81.7</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">27.5</td>
            <td style="text-align: center;">58.3</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B-Ins</td>
            <td style="text-align: center;">60.7</td>
            <td style="text-align: center;">52.1</td>
            <td style="text-align: center;">26.9</td>
            <td style="text-align: center;">74.3</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">62.0</td>
            <td style="text-align: center;">28.2</td>
            <td style="text-align: center;">58.2</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Cambrian-1-8B</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">42.7</td>
            <td style="text-align: center;">14.7</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">74.6</td>
            <td style="text-align: center;">48</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">49.0</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Llava-CoT-11B</td>
            <td style="text-align: center;">57.6</td>
            <td style="text-align: center;">48.9</td>
            <td style="text-align: center;">18.5</td>
            <td style="text-align: center;">75.2</td>
            <td style="text-align: center;">75.0</td>
            <td style="text-align: center;">60.3</td>
            <td style="text-align: center;">24.2</td>
            <td style="text-align: center;">54.8</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Molmo-7B-D</td>
            <td style="text-align: center;">50.5</td>
            <td style="text-align: center;">45.3</td>
            <td style="text-align: center;">18.9</td>
            <td style="text-align: center;">74.1</td>
            <td style="text-align: center;">73.6</td>
            <td style="text-align: center;">58.0</td>
            <td style="text-align: center;">21.5</td>
            <td style="text-align: center;">51.6</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">47.3</td>
            <td style="text-align: center;">16.8</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">80.5</td>
            <td style="text-align: center;">58.8</td>
            <td style="text-align: center;">26.9</td>
            <td style="text-align: center;">56.1</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">61.7</td>
            <td style="text-align: center;">48.8</td>
            <td style="text-align: center;">18.7</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">80.8</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">26.2</td>
            <td style="text-align: center;">63.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B (SI)</td>
            <td style="text-align: center;">55.4</td>
            <td style="text-align: center;">49.4</td>
            <td style="text-align: center;">26.0</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">60.6</td>
            <td style="text-align: center;">35.0</td>
            <td style="text-align: center;">67.6</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">63.0</td>
            <td style="text-align: center;">50.8</td>
            <td style="text-align: center;">25.3</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">83.4</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">34.2</td>
            <td style="text-align: center;">67.6</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source<br>(~10B Scale)</td>
            <td style="text-align: center;vertical-align: middle">+1.3</td>
            <td style="text-align: center;vertical-align: middle">+1.9</td>
            <td style="text-align: center;vertical-align: middle">+7.1</td>
            <td style="text-align: center;vertical-align: middle">+0.6</td>
            <td style="text-align: center;vertical-align: middle">+2.6</td>
            <td style="text-align: center;vertical-align: middle">+2.0</td>
            <td style="text-align: center;vertical-align: middle">+8.1</td>
            <td style="text-align: center;vertical-align: middle">+4.4</td>
        </tr>
                
    </tbody>
</table>
<caption>Table1: Main results on Multi-discipline Knowledge and Mathematical Reasoning benchmarks. Results are taken
    from official papers or blogs when available; otherwise, we use lmms-eval for evaluation. The models highlighted
    in gray represent closed-source models, those in purple-blue are open-weight in terms of model weights but lack
    open-source training data or code, and those in green have fully open-source training details, including weights,
    data, and code</caption>

<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="4" style="text-align: center;">Chart & Doc Understanding</th>
            <th colspan="3" style="text-align: center;">Multimodal Interactions & Preferences</th>
        </tr>
        <tr>
            <th style="text-align: center;">AI2D<br>test</th>
            <th style="text-align: center;">ChartQA<br>test</th>
            <th style="text-align: center;">InfoVQA<br>test</th>
            <th style="text-align: center;">DocVQA<br>test</th>
            <th style="text-align: center;">RealWorldQA<br>test</th>
            <th style="text-align: center;">WildVision<br>0617</th>
            <th style="text-align: center;">L-Wilder<br>small</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">94.2</td>
            <td style="text-align: center;">85.7</td>
            <td style="text-align: center;">79.2</td>
            <td style="text-align: center;">92.8</td>
            <td style="text-align: center;">76.5</td>
            <td style="text-align: center;">89.4</td>
            <td style="text-align: center;">85.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Gemini-1.5-Pro</td>
            <td style="text-align: center;">94.4</td>
            <td style="text-align: center;">87.2</td>
            <td style="text-align: center;">81.0</td>
            <td style="text-align: center;">93.1</td>
            <td style="text-align: center;">70.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Claude-3.5-Sonnet</td>
            <td style="text-align: center;">94.7</td>
            <td style="text-align: center;">90.8</td>
            <td style="text-align: center;">49.7</td>
            <td style="text-align: center;">95.2</td>
            <td style="text-align: center;">59.9</td>
            <td style="text-align: center;">50.0</td>
            <td style="text-align: center;">83.1</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL2-LLaMa3-76B</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">82.0</td>
            <td style="text-align: center;">94.1</td>
            <td style="text-align: center;">72.7</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-72B-Ins</td>
            <td style="text-align: center;">88.1</td>
            <td style="text-align: center;">88.3</td>
            <td style="text-align: center;">84.5</td>
            <td style="text-align: center;">96.5</td>
            <td style="text-align: center;">77.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B (SI)</td>
            <td style="text-align: center;">85.1</td>
            <td style="text-align: center;">84.9</td>
            <td style="text-align: center;">74.6</td>
            <td style="text-align: center;">91.8</td>
            <td style="text-align: center;">73.8</td>
            <td style="text-align: center;">49.5</td>
            <td style="text-align: center;">72.9</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">85.6</td>
            <td style="text-align: center;">83.7</td>
            <td style="text-align: center;">74.9</td>
            <td style="text-align: center;">91.3</td>
            <td style="text-align: center;">71.9</td>
            <td style="text-align: center;">52.3</td>
            <td style="text-align: center;">72.0</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>MiniCPM-V-2.6-7B</td>
            <td style="text-align: center;">82.1</td>
            <td style="text-align: center;">82.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">90.8</td>
            <td style="text-align: center;">65.0</td>
            <td style="text-align: center;">11.7</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternLM-XComp-2.5-7B</td>
            <td style="text-align: center;">81.5</td>
            <td style="text-align: center;">82.2</td>
            <td style="text-align: center;">70.0</td>
            <td style="text-align: center;">90.9</td>
            <td style="text-align: center;">67.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">61.4</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Llama-3.2-11B-Vision-Ins</td>
            <td style="text-align: center;">77.3</td>
            <td style="text-align: center;">83.4</td>
            <td style="text-align: center;">65.0</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">63.3</td>
            <td style="text-align: center;">49.7</td>
            <td style="text-align: center;">62.0</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">83.8</td>
            <td style="text-align: center;">83.3</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">91.6</td>
            <td style="text-align: center;">64.4</td>
            <td style="text-align: center;">51.5</td>
            <td style="text-align: center;">62.5</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B-Ins</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">76.5</td>
            <td style="text-align: center;">94.5</td>
            <td style="text-align: center;">70.1</td>
            <td style="text-align: center;">44</td>
            <td style="text-align: center;">66.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Cambrian-1-8B</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">41.6</td>
            <td style="text-align: center;">77.8</td>
            <td style="text-align: center;">64.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Llava-CoT-11B</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">67.0</td>
            <td style="text-align: center;">44.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">65.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Molmo-7B-D</td>
            <td style="text-align: center;">81.0</td>
            <td style="text-align: center;">84.1</td>
            <td style="text-align: center;">72.6</td>
            <td style="text-align: center;">92.2</td>
            <td style="text-align: center;">70.7</td>
            <td style="text-align: center;">40</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">81.6</td>
            <td style="text-align: center;">78.8</td>
            <td style="text-align: center;">65.3</td>
            <td style="text-align: center;">86.9</td>
            <td style="text-align: center;">65.5</td>
            <td style="text-align: center;">39.2</td>
            <td style="text-align: center;">69.1</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">81.4</td>
            <td style="text-align: center;">80.0</td>
            <td style="text-align: center;">68.8</td>
            <td style="text-align: center;">87.5</td>
            <td style="text-align: center;">66.3</td>
            <td style="text-align: center;">53.8</td>
            <td style="text-align: center;">67.8</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B (SI)</td>
            <td style="text-align: center;">83.4</td>
            <td style="text-align: center;">85.9</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">93.8</td>
            <td style="text-align: center;">71.3</td>
            <td style="text-align: center;">51.9</td>
            <td style="text-align: center;">71.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">84.0</td>
            <td style="text-align: center;">86.2</td>
            <td style="text-align: center;">73.1</td>
            <td style="text-align: center;">93.7</td>
            <td style="text-align: center;">69.9</td>
            <td style="text-align: center;">51.1</td>
            <td style="text-align: center;">70.8</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source<br>(~10B Scale)</td>
            <td style="text-align: center;vertical-align: middle">+2.4</td>
            <td style="text-align: center;vertical-align: middle">+2.1</td>
            <td style="text-align: center;vertical-align: middle">+2.2</td>
            <td style="text-align: center;vertical-align: middle">+1.6</td>
            <td style="text-align: center;vertical-align: middle">+0.6</td>
            <td style="text-align: center;vertical-align: middle">-1.9</td>
            <td style="text-align: center;vertical-align: middle">+2.2</td>
        </tr>
                
    </tbody>
</table>
<caption>Table2: Main results on Chart, Diagram, and Document Understanding, and Real-world Multimodal Interactions and Human Preferences benchmarks. </caption>


<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="8" style="text-align: center;">Multi-Image and Video</th>
        </tr>
        <tr>
            <th style="text-align: center;">MuirBench<br>test</th>
            <th style="text-align: center;">MEGABench<br>test</th>
            <th style="text-align: center;">EgoSchema<br>test</th>
            <th style="text-align: center;">PerceptionTest<br>test</th>
            <th style="text-align: center;">SeedBench<br>video</th>
            <th style="text-align: center;">MLVU<br>dev</th>
            <th style="text-align: center;">MVBench<br>test</th>
            <th style="text-align: center;">VideoMME<br>w/o subs</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">68.0</td>
            <td style="text-align: center;">54.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">64.6</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">71.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4v</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">60.5</td>
            <td style="text-align: center;">49.2</td>
            <td style="text-align: center;">43.5</td>
            <td style="text-align: center;">59.9</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B (SI)</td>
            <td style="text-align: center;">33.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">64.8</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">54.8</td>
            <td style="text-align: center;">33.8</td>
            <td style="text-align: center;">62.0</td>
            <td style="text-align: center;">66.9</td>
            <td style="text-align: center;">62.1</td>
            <td style="text-align: center;">66.4</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">66.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">27.7</td>
            <td style="text-align: center;">54.2</td>
            <td style="text-align: center;">57.4</td>
            <td style="text-align: center;">54.9</td>
            <td style="text-align: center;">30.2</td>
            <td style="text-align: center;">66.4</td>
            <td style="text-align: center;">54.0</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B-Ins</td>
            <td style="text-align: center;">41.6</td>
            <td style="text-align: center;">36.0</td>
            <td style="text-align: center;">66.7</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">55.3</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">67</td>
            <td style="text-align: center;">63.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">32.7</td>
            <td style="text-align: center;">22.1</td>
            <td style="text-align: center;">52.9</td>
            <td style="text-align: center;">54.9</td>
            <td style="text-align: center;">51.1</td>
            <td style="text-align: center;">60.2</td>
            <td style="text-align: center;">51.2</td>
            <td style="text-align: center;">55.0</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">41.8</td>
            <td style="text-align: center;">23.9</td>
            <td style="text-align: center;">60.1</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">56.9</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">56.7</td>
            <td style="text-align: center;">58.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">55.1</td>
            <td style="text-align: center;">28.2</td>
            <td style="text-align: center;">58.5</td>
            <td style="text-align: center;">59.3</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">59.1</td>
            <td style="text-align: center;">58.8</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source<br>(~10B Scale)</td>
            <td style="text-align: center;vertical-align: middle">+13.3</td>
            <td style="text-align: center;vertical-align: middle">+4.3</td>
            <td style="text-align: center;vertical-align: middle">-1.6</td>
            <td style="text-align: center;vertical-align: middle">+2.2</td>
            <td style="text-align: center;vertical-align: middle">+0.2</td>
            <td style="text-align: center;vertical-align: middle">+0</td>
            <td style="text-align: center;vertical-align: middle">+2.4</td>
            <td style="text-align: center;vertical-align: middle">+0.6</td>
        </tr>
                
    </tbody>
</table>
<caption>Table 3: Main results on Multi-Image and Video benchmarks.</caption>




<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Effect Of Data Scale</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            As shown in Figure 4, performance is tracked across benchmarks with training dataset size increasing in 2-million-sample intervals. Results are compared to three leading models: Llava-OneVision-7B & 72B and Llava-CoT. The findings demonstrate a positive correlation between training data scale and performance, indicating that diverse instruction data improves the model's ability to handle complex tasks. 
                        </p>

<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths"> -->
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/mammoth_vl_scale.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; color: #888888;font-weight: normal;">
                            Figure 4: Scaling effects of MAmmoTH-VL-8B on eight multimodal evaluation datasets. A simple rewriting approach using open models improves the quality of visual instruction data by eliciting chain-of-thought (CoT) reasoning. Training on this rewritten data demonstrates significant performance gains through increased model scale. Llava-OneVision-7B & 72B and Llava-CoT are included as references.
                        </h2>
                    </div>
                <!-- </div>
            </div>
        </div>
    </div>
</section> -->


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Effect Of Rewrite Model</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To assess the effect of model size on the quality of rewritten data, we conduct experiments using four models trained on a dataset of 500K samples. The first model is trained on the original dataset. The second model use data rewritten by InternVL2-Llama3-76B and Meta-Llama-3-70B-Instruct. The third model is trained on data rewritten by Qwen2-VL-7B-Instruct and Qwen2.5-7B-Instruct. The fourth model is trained on data rewritten by InternVL2-8B and InternLM2.5-7B. Among these, Qwen2.5-72B-Instruct, Qwen2.5-7B-Instruct, and InternLM2.5-7B are employed solely for rewriting caption data, while InternVL2-Llama3-76B, Qwen2-VL-7B-Instruct, and InternVL2-8B are used for data filtering.                        </p>
                        <p>
                            As shown in Figure 5, our analysis reveals distinct patterns in model performance across different task categories. For knowledge & reasoning tasks, models trained on data rewritten by smaller models (approximately 7B parameters) achieve performance comparable to those using larger model rewrites. However, the impact of data rewriting varies significantly by task type. For chart and document-related tasks, rewriting with smaller models actually leads to performance degradation, while larger models provide modest improvements. This suggests that sophisticated visual understanding capabilities of larger models are crucial for effectively rewriting such data. In contrast, Multi Interact & Preference tasks demonstrate a clear correlation with model scale, where larger models excel in handling these complex scenarios that demand subtle understanding and nuanced preference modeling.
                        </p>

<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths"> -->
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/ablation_rewrite_model.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 5: Performance of data rewritten by different models on three benchmark subsets.
                        </h2>
                    </div>
                <!-- </div>
            </div>
        </div>
    </div>
</section> -->

<!-- End image3 carousel -->


<!-- Image6 carousel -->
                                
<!-- End image6 carousel -->




<!--ending WEBINSTRUCT-->

   

<!-- 
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overall Results</h2>
                        <div class="item">
                            Your image here -->
                            <!-- <img src="static/images/overall_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle has-text-centered">
                                Figure 2: Overall results of ü¶£MAmmoTH on the in-domain and out-of-domain datasets.
                            </h2>
                            <p>
                                Overall, we can see that MAmmoTH and MAmmoTH-Coder are able to outperform the SoTA model at different scales. In general, the performance gain for OOD datasets is more significant than IND datasets. These results show us the potential of our models as
                                a mathematical generalist. On several datasets, MAmmoTH-Coder-34B and MAmmoTH-70B are even surpassing closed-source LLMs (see more break down results below).
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->


    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Where does the gain come from?</h2>
                        <div class="item"> -->
                            <!-- Your image here -->
                            <!-- <img src="static/images/ablation_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 3: Investigation of the influence of CoT \& PoT hybrid training on the 7B Llama-2 model. Key insights include: 1) The SoTA model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance within its domains but struggles
                                in OOD scenarios; 2) Diverse data sources in MathInstruct enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms fine-tuning on the CoT subsets; 4) Hybrid training yields the best-performing
                                model.
                            </h2>
                            <p>
                                In order to better understand what factors contribute to the great gain of ü¶£MAmmoTH over existing baselines, we set up a group of control experiments in the Figure 3. We study the following setups:
                                <ol>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - CoT):</b> This experiment aims to understand how much our curated CoT data could improve the generalization over the SoTA model WizardMath trained specifically on GSM + MATH. As can be seen,
                                        while sacrificing accuracy on GSM + MATH by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%. </li>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - PoT):</b> This experiment aims to understand the advantage of our PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall accuracy from 27% to 37.5%.
                                        This ablation reflects the importance of unlocking the program generation capabilities of our model.</li>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - Hybrid):</b> We further combine CoT and PoT as the hybrid training data to achieve the best overall performance of 45.4%. This combined gain comes from two aspects:
                                        <ul style="list-style-type: disc;">
                                            <li>
                                                The CoT subset can help maintain the generic language-based reasoning skills to handle scenarios where PoT cannot handle well, e.g., the multi-choice questions in AQuA, SAT, and MMLU.
                                            </li>
                                            <li>
                                                The PoT subset can teach the model how to utilize Python APIs to solve complex math problems with high precision, e.g., the MATH problems requiring complex computation.
                                            </li>
                                        </ul>
                                    </li>
                                </ol> -->





                                <!-- We put some case studies in Appendix \ref{sec:case_study} to demonstrate the respective advantages of PoT and CoT in solving different types of math problems. To summarize, we attribute our substantial gain to: 1) diverse data sources covering different math fields and complexity levels and 2) a hybrid of CoT \& PoT instruction tuning strategy.  -->

                            <!-- </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->


<section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">Reference</h2>
            Please kindly cite our paper if you use our code, data, models or results:
            <br><br>
            <pre><code>
@article{guo2024mammothvlelicitingmultimodalreasoning,
    title={MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale}, 
    author={Jarvis Guo and Tuney Zheng and Yuelin Bai and Bo Li and Yubo Wang and King Zhu and Yizhi Li and Graham Neubig and Wenhu Chen and Xiang Yue},
    year={2024},
    eprint={2412.05237},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2412.05237}, 
}
          </code></pre>
        </div>
</section>


<footer class="footer">
    <!-- <div class="container"> -->
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://tiger-ai-lab.github.io/MAmmoTH/">MAmmoTH</a> and <a href="https://mmmu.github.io/">MMMU</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    <!-- </div> -->
  
  </footer>

<!--table4-->
</html>
