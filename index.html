<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
    <meta property="og:url" content="URL OF THE WEBSITE" />
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>MAmmoTH-VL</title>
    <link rel="icon" type="image/x-icon" href="static/images/mammoth_vl.png">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script type="text/javascript" src="static/js/sort-table.js" defer></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./data/results/data_setting.js" defer></script>
  <script src="./data/results/model_scores.js" defer></script>
  <script src="./visualizer/data/data_public.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
        <a class="navbar-item" href="https://opencodeinterpreter.github.io/">
            <b>OpenCodeInterpreter</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
            </a>
          <a class="navbar-item" href="https://mmmu-benchmark.github.io/">
            <b>MMMU</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://tiger-ai-lab.github.io/MAmmoTH/">
            <b>MAmmoTH</b> <p style="font-size:18px; display: inline; margin-left: 5px;">üî•</p>
          </a>
          <a class="navbar-item" href="https://osu-nlp-group.github.io/TableLlama/">
            TableLlama 
            <a class="navbar-item" href="https://osu-nlp-group.github.io/MagicBrush/">
              MagicBrush
            </a>
            <a class="navbar-item" href="https://osu-nlp-group.github.io/Mind2Web/">
              Mind2Web
            </a>
          </a>
          
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <!-- <h1 class="title is-1 publication-title">ü¶£ MAmmoTH-VL: <br>Scaling Synthetic Multimodal Instruction Data with Open Models</h1> -->
                        <h1 class="title is-1 publication-title">
                            <img src="static/images/mammoth_vl.png" alt="Mammoth Icon" style="vertical-align: -2px; width: 60px; height: 40px;"> MAmmoTH-VL: <br>Scaling Synthetic Multimodal Instruction Data with Open Models
                          </h1>
                          
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                <span class="author-block">
                    <sup>1</sup><a href="" target="_blank">Jarvis Guo</a><sup>*</sup>,</span>
                <span class="author-block">
                    <sup>1</sup><a href="" target="_blank">Tuney Zheng</a><sup>*</sup>,</span>
                <span class="author-block">
                        <sup>1</sup><a href="" target="_blank">Yuelin Bai</a>,</span>
                <span class="author-block">
                        <sup>3</sup><a href="" target="_blank">Bo Li</a>
                </span>
                <br>
                <span class="author-block">
                    <sup>4</sup><a href="" target="_blank">Yubo Wang</a>,</span>
                <span class="author-block">
                    <sup>1</sup><a href="" target="_blank">King Zhu</a>,</span>
                <span class="author-block">
                    <sup>5</sup><a href="" target="_blank">Yizhi Li</a>,
                </span>
                <br>
                <span class="author-block">
                    <sup>2</sup><a href="" target="_blank">Graham Neubig</a>,</span>
                <span class="author-block">
                    <sup>4</sup><a href="https://wenhuchen.github.io/" target="_blank">Wenhu Chen</a>,</span>
                <span class="author-block">
                    <sup>2</sup><a href="https://xiangyue9607.github.io/" target="_blank">Xiang Yue</a>,
                </span>
                        </div>



                <div class="is-size-5 publication-authors">
                    <span class="author-block">
                        <sup>1</sup>M-A-P,
                        <sup>2</sup>Carnegie Mellon University,
                        <sup>3</sup>Nanyang Technological University,
                        <br>
                        <sup>4</sup>University of Waterloo,
                        <sup>5</sup>The University of Manchester
                        <br>
                    <!-- <span class="eql-cntrb"><small><sup>*</sup>Xiang Yue and Wenhu Chen are the leading authors of the project. They contributed equally to this project.</small></span> -->
                            <span class="author-block"><a href="mailto:xyue2@andrew.cmu.edu">xyue2@andrew.cmu.edu</a>
                      <!-- , <a href="mailto:wenhuchen@uwaterloo.ca">wenhuchen@uwaterloo.ca</a> </span> -->

                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                          <a href="https://huggingface.co/datasets/MMSFT/MAmmoTH-VL-12M" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            ü§ó
                          </span>
                                <span>Dataset</span>
                                </a>
                                </span>

                                <!-- Supplementary PDF link -->
                                <span class="link-block">
                        <a href="https://huggingface.co/MMSFT/MAmmoTH-VL-8B" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          ü§ó
                        </span>
                                <span>Models</span>
                                </a>
                                </span>

                                <!-- Github link -->
                                <span class="link-block">
                      <a href="https://github.com/TIGER-AI-Lab/MAmmoTH2" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                                <span>Code</span>
                                </a>
                                </span>

                                <!-- ArXiv abstract Link -->
                                <span class="link-block">
                    <a href="https://arxiv.org/pdf/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>



    <!-- Paper abstract -->
   <!-- Paper Abstract Section -->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Multimodal Large Language Models (MLLMs) have integrated modality alignment and visual instruction tuning to excel in tasks like captioning and visual question-answering. Despite their potential, MLLMs often struggle with real-world applications due to the homogeneity and simplicity of training datasets. Addressing these challenges, we propose a simple, scalable yet effective approach that leverages open-source models for synthesizing high-quality, diverse instruction training data, aligning closely with human preferences and complex real-world scenarios. We introduce the MAmmoTH-VL-12M dataset, a collection of 12 million multimodal instructions, across 12 task domains. Experimental results show that training an MLLM, MAmmoTH-VL-8B on this dataset enhances performance in various visual reasoning, OCR, chart understanding, document understanding, and interaction tasks. Our method demonstrates a cost-effective alternative for high-quality multimodal data creation in the open-source community.                        </p>

<p>
    <!-- Image1 carousel -->
        
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <div class="item">
                            <img src="static/images/mammoth_vl_method.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                            <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                                Figure 1: Overview of MAmmoTH-VL.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    
    <!-- End image1 carousel -->
    
    <p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End of Paper Abstract Section -->





<!--begining introduction-->

<!-- <section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h2 class="title is-3">Introduction</h2>
                    <div class="content has-text-justified">
                        <p>
                            The rapid development of multimodal large language models (MLLMs) represents a significant advancement in the field of AI. Among the techniques driving this progress in the open-source community, the ‚Äúconnector-training‚Äù approaches, such as LLaVA, have gained prominence due to their simplicity and effectiveness. Connector training integrates a pretrained visual encoder, such as a Vision Transformer (ViT), with a pretrained large language model (LLM) via a lightweight adapter module or projection layer. This alignment enables the model to bridge textual and visual modalities, and fine-tuning on high-quality visual instruction data enhances multimodal capabilities for tasks ranging from image captioning to complex reasoning. However, a central challenge is the need for high-quality supervised fine-tuning (SFT) data, which is essential for adapting pretrained components to multimodal learning tasks.
                        </p>
                            
                        <p>
                            Generating such large-scale, high-quality multimodal SFT datasets poses significant challenges, particularly in open-source contexts with limited resources. Existing approaches typically rely on three methods: (1) human annotation, which is precise but expensive and labor-intensive, (2) repurposing existing academic datasets, such as visual question-answering datasets, which are accessible but lack diversity and in-depth reasoning content, and (3) querying proprietary models like GPT-4, which can produce high-quality data but come with high costs and licensing issues. To address this, we propose a simple, scalable, and cost-effective methodology for constructing a high-quality multimodal dataset, leveraging only open models. Our approach includes collecting and categorizing a diverse set of images from internet sources, followed by task-specific data augmentation and rewriting using open-weight LLMs and MLLMs, and concluding with rigorous quality filtering to remove irrelevant content and alleviate model bias.
                        </p>
                            
                        <p>
                            Our experimental results demonstrate the effectiveness of this approach. Using a curated dataset of 12 million visual instruction entries, we train an MLLM model based on the LLaVA-OneVision architecture. Evaluation across various downstream tasks reveals that our dataset significantly improves model performance, particularly in complex reasoning tasks, accuracy in visual question answering, and alignment between visual and textual content. By utilizing open models throughout the process, we are able to accelerate the development of multimodal large language models without relying on proprietary resources. The results highlight the potential of our method to advance the science of open MLLMs while maintaining accessibility and scalability.
                        </p>
                            
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <img src="static/images/mammoth_vl_method.jpg" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 2: 
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
 -->


<!--end begining introduction-->
<!--begining WEBINSTRUCT-->
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Data Generation Pipeline</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            Existing efforts have demonstrated the potential of visual instruction tuning, they often rely on resource-intensive approaches, such as human annotations or proprietary models, limiting their scalability and accessibility, especially in open-source contexts. To solve this, we present a simple, scalable, and cost-effective data generation pipeline to construct 12M high-quality samples. Our approach includes three key steps: 
                        </p>
                        <p>
                            (1) comprehensive open-source data curation and categorization
                        </p>
                        <p>
                            (2) task-aware data augmentation and rewriting with open models
                        </p>
                        <p>
                            (3) rigorous quality filtering to eliminate hallucinated or irrelevant content. 
                        </p>
<!-- Image3 carousel -->
    
<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/mammoth_vl_12M.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 2: Overview of MAmmoTH-VL-12M.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Overall Results</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To reveal the generality and effectiveness of model, we comprehensively evaluate our model across different modalities, including single-image, multi-image, and video benchmarks.We denote the model checkpoint trained after the single-image stage and one-vision stage as MAmmoTH-VL-8B(SI) and MAmmoTH-VL-8B.We conduct standardized, reproducible evaluations of our model across all benchmarks using LMMs-Eval. To ensure a fair comparison with other VLMs, we primarily report results from the original papers. When results are unavailable, we onboard the models in LMMs-Eval and evaluate them using consistent settings. All results are reported using greedy decoding and zero-shot settings unless otherwise specified.    
                        </p>
<!-- Image3 carousel -->



<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="8" style="text-align: center;">Multi-Discipline Knowledge and Mathematical Reasoning</th>
        </tr>
        <tr>
            <th style="text-align: center;">MMStar<br>test</th>
            <th style="text-align: center;">MMMU<br>val</th>
            <th style="text-align: center;">MMMU-Pro<br>vision</th>
            <th style="text-align: center;">SeedBench<br>test</th>
            <th style="text-align: center;">MMBench<br>en-test</th>
            <th style="text-align: center;">MMVet<br>test</th>
            <th style="text-align: center;">MathVerse<br>mini-vision</th>
            <th style="text-align: center;">MathVista<br>testmini</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">49.7</td>
            <td style="text-align: center;">76.2</td>
            <td style="text-align: center;">82.1</td>
            <td style="text-align: center;">76.2</td>
            <td style="text-align: center;">50.2</td>
            <td style="text-align: center;">63.8</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4v</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">56.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">49.9</td>
            <td style="text-align: center;">77.0</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">32.8</td>
            <td style="text-align: center;">49.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Gemini-1.5-Pro</td>
            <td style="text-align: center;">59.1</td>
            <td style="text-align: center;">62.2</td>
            <td style="text-align: center;">44.4</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">73.9</td>
            <td style="text-align: center;">64.0</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">63.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Claude-3.5-Sonnet</td>
            <td style="text-align: center;">62.2</td>
            <td style="text-align: center;">68.3</td>
            <td style="text-align: center;">48.0</td>
            <td style="text-align: center;">72.2</td>
            <td style="text-align: center;">79.7</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">67.7</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL2-LLaMa3-76B</td>
            <td style="text-align: center;">67.1</td>
            <td style="text-align: center;">58.2</td>
            <td style="text-align: center;">38.0</td>
            <td style="text-align: center;">77.6</td>
            <td style="text-align: center;">86.5</td>
            <td style="text-align: center;">64.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">65.5</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-72B</td>
            <td style="text-align: center;">68.6</td>
            <td style="text-align: center;">64.5</td>
            <td style="text-align: center;">37.1</td>
            <td style="text-align: center;">77.9</td>
            <td style="text-align: center;">86.9</td>
            <td style="text-align: center;">73.9</td>
            <td style="text-align: center;">37.3</td>
            <td style="text-align: center;">70.5</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B(Single Image)</td>
            <td style="text-align: center;">65.2</td>
            <td style="text-align: center;">57.4</td>
            <td style="text-align: center;">26.0</td>
            <td style="text-align: center;">77.6</td>
            <td style="text-align: center;">86.6</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">37.7</td>
            <td style="text-align: center;">66.5</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">66.1</td>
            <td style="text-align: center;">56.8</td>
            <td style="text-align: center;">24.0</td>
            <td style="text-align: center;">78.0</td>
            <td style="text-align: center;">85.9</td>
            <td style="text-align: center;">63.7</td>
            <td style="text-align: center;">39.1</td>
            <td style="text-align: center;">67.5</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>MiniCPM-V-2.6-7B</td>
            <td style="text-align: center;">57.5</td>
            <td style="text-align: center;">49.8</td>
            <td style="text-align: center;">21.7</td>
            <td style="text-align: center;">74.0</td>
            <td style="text-align: center;">81.5</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">60.6</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternLM-XComp-2.5-7B</td>
            <td style="text-align: center;">59.9</td>
            <td style="text-align: center;">42.9</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">74.4</td>
            <td style="text-align: center;">51.7</td>
            <td style="text-align: center;">20.0</td>
            <td style="text-align: center;">59.6</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">49.3</td>
            <td style="text-align: center;">25.4</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">81.7</td>
            <td style="text-align: center;">60.0</td>
            <td style="text-align: center;">27.5</td>
            <td style="text-align: center;">58.3</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B</td>
            <td style="text-align: center;">60.7</td>
            <td style="text-align: center;">54.1</td>
            <td style="text-align: center;">26.9</td>
            <td style="text-align: center;">74.3</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">62.0</td>
            <td style="text-align: center;">28.2</td>
            <td style="text-align: center;">58.2</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Cambrian-1-8B</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">42.7</td>
            <td style="text-align: center;">14.7</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">74.6</td>
            <td style="text-align: center;">48</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">49.0</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Molmo-7B-D</td>
            <td style="text-align: center;">50.5</td>
            <td style="text-align: center;">45.3</td>
            <td style="text-align: center;">18.9</td>
            <td style="text-align: center;">74.1</td>
            <td style="text-align: center;">73.6</td>
            <td style="text-align: center;">58.0</td>
            <td style="text-align: center;">21.5</td>
            <td style="text-align: center;">51.6</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">47.3</td>
            <td style="text-align: center;">16.8</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">80.5</td>
            <td style="text-align: center;">58.8</td>
            <td style="text-align: center;">26.9</td>
            <td style="text-align: center;">56.1</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">61.7</td>
            <td style="text-align: center;">48.8</td>
            <td style="text-align: center;">18.7</td>
            <td style="text-align: center;">75.4</td>
            <td style="text-align: center;">80.8</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">26.2</td>
            <td style="text-align: center;">63.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B (SI)</td>
            <td style="text-align: center;">55.4</td>
            <td style="text-align: center;">49.4</td>
            <td style="text-align: center;">26.0</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">60.6</td>
            <td style="text-align: center;">35.0</td>
            <td style="text-align: center;">67.6</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">63.0</td>
            <td style="text-align: center;">50.8</td>
            <td style="text-align: center;">25.3</td>
            <td style="text-align: center;">76.0</td>
            <td style="text-align: center;">83.4</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">34.2</td>
            <td style="text-align: center;">67.6</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source</td>
            <td style="text-align: center;">+1.3</td>
            <td style="text-align: center;">+2.0</td>
            <td style="text-align: center;">+7.1</td>
            <td style="text-align: center;">+0.6</td>
            <td style="text-align: center;">+2.6</td>
            <td style="text-align: center;">+3.7</td>
            <td style="text-align: center;">+8.1</td>
            <td style="text-align: center;">+4.4</td>
        </tr>
                
    </tbody>
</table>
<caption>Table1: Main results on Multi-discipline Knowledge and Mathematical Reasoning benchmarks. Results are taken
    from official papers or blogs when available; otherwise, we use lmms-eval for evaluation. The models highlighted
    in gray represent closed-source models, those in purple-blue are open-weight in terms of model weights but lack
    open-source training data or code, and those in green have fully open-source training details, including weights,
    data, and code</caption>

<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="4" style="text-align: center;">Chart & Doc Understanding</th>
            <th colspan="3" style="text-align: center;">Multimodal Interactions & Preferences</th>
        </tr>
        <tr>
            <th style="text-align: center;">AI2D<br>test</th>
            <th style="text-align: center;">ChartQA<br>test</th>
            <th style="text-align: center;">InfoVQA<br>test</th>
            <th style="text-align: center;">DocVQA<br>test</th>
            <th style="text-align: center;">RealWorldQA<br>test</th>
            <th style="text-align: center;">WildVision<br>0617</th>
            <th style="text-align: center;">L-Wilder<br>small</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">94.2</td>
            <td style="text-align: center;">85.7</td>
            <td style="text-align: center;">79.2</td>
            <td style="text-align: center;">92.8</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">89.4</td>
            <td style="text-align: center;">85.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4v</td>
            <td style="text-align: center;">78.2</td>
            <td style="text-align: center;">78.5</td>
            <td style="text-align: center;">75.1</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">61.4</td>
            <td style="text-align: center;">80.0</td>
            <td style="text-align: center;">81.0</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Gemini-1.5-Pro</td>
            <td style="text-align: center;">94.4</td>
            <td style="text-align: center;">87.2</td>
            <td style="text-align: center;">81.0</td>
            <td style="text-align: center;">93.1</td>
            <td style="text-align: center;">70.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>Claude-3.5-Sonnet</td>
            <td style="text-align: center;">94.7</td>
            <td style="text-align: center;">90.8</td>
            <td style="text-align: center;">49.7</td>
            <td style="text-align: center;">95.2</td>
            <td style="text-align: center;">59.9</td>
            <td style="text-align: center;">50.0</td>
            <td style="text-align: center;">83.1</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL2-LLaMa3-76B</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">88.4</td>
            <td style="text-align: center;">82.0</td>
            <td style="text-align: center;">94.1</td>
            <td style="text-align: center;">72.7</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-72B</td>
            <td style="text-align: center;">88.1</td>
            <td style="text-align: center;">88.3</td>
            <td style="text-align: center;">84.5</td>
            <td style="text-align: center;">96.5</td>
            <td style="text-align: center;">77.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B(Single Image)</td>
            <td style="text-align: center;">85.1</td>
            <td style="text-align: center;">84.9</td>
            <td style="text-align: center;">74.6</td>
            <td style="text-align: center;">91.8</td>
            <td style="text-align: center;">73.8</td>
            <td style="text-align: center;">49.5</td>
            <td style="text-align: center;">72.9</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">85.6</td>
            <td style="text-align: center;">83.7</td>
            <td style="text-align: center;">74.9</td>
            <td style="text-align: center;">91.3</td>
            <td style="text-align: center;">71.9</td>
            <td style="text-align: center;">52.3</td>
            <td style="text-align: center;">72.0</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>MiniCPM-V-2.6-7B</td>
            <td style="text-align: center;">82.1</td>
            <td style="text-align: center;">82.4</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">90.8</td>
            <td style="text-align: center;">65.0</td>
            <td style="text-align: center;">11.7</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternLM-XComp-2.5-7B</td>
            <td style="text-align: center;">81.5</td>
            <td style="text-align: center;">82.2</td>
            <td style="text-align: center;">70.0</td>
            <td style="text-align: center;">90.9</td>
            <td style="text-align: center;">67.8</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">61.4</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">83.8</td>
            <td style="text-align: center;">83.3</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">91.6</td>
            <td style="text-align: center;">64.4</td>
            <td style="text-align: center;">51.5</td>
            <td style="text-align: center;">62.5</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">83.0</td>
            <td style="text-align: center;">76.5</td>
            <td style="text-align: center;">94.5</td>
            <td style="text-align: center;">70.1</td>
            <td style="text-align: center;">44</td>
            <td style="text-align: center;">66.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Cambrian-1-8B</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">73.3</td>
            <td style="text-align: center;">41.6</td>
            <td style="text-align: center;">77.8</td>
            <td style="text-align: center;">64.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>Molmo-7B-D</td>
            <td style="text-align: center;">81.0</td>
            <td style="text-align: center;">84.1</td>
            <td style="text-align: center;">72.6</td>
            <td style="text-align: center;">92.2</td>
            <td style="text-align: center;">70.7</td>
            <td style="text-align: center;">40</td>
            <td style="text-align: center;">-</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">81.6</td>
            <td style="text-align: center;">78.8</td>
            <td style="text-align: center;">65.3</td>
            <td style="text-align: center;">86.9</td>
            <td style="text-align: center;">65.5</td>
            <td style="text-align: center;">39.2</td>
            <td style="text-align: center;">69.1</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">81.4</td>
            <td style="text-align: center;">80.0</td>
            <td style="text-align: center;">68.8</td>
            <td style="text-align: center;">87.5</td>
            <td style="text-align: center;">66.3</td>
            <td style="text-align: center;">53.8</td>
            <td style="text-align: center;">67.8</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B (SI)</td>
            <td style="text-align: center;">83.4</td>
            <td style="text-align: center;">85.9</td>
            <td style="text-align: center;">74.8</td>
            <td style="text-align: center;">93.8</td>
            <td style="text-align: center;">71.3</td>
            <td style="text-align: center;">51.9</td>
            <td style="text-align: center;">71.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">84.0</td>
            <td style="text-align: center;">86.2</td>
            <td style="text-align: center;">73.1</td>
            <td style="text-align: center;">93.7</td>
            <td style="text-align: center;">69.9</td>
            <td style="text-align: center;">51.1</td>
            <td style="text-align: center;">70.8</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source</td>
            <td style="text-align: center;">+2.4</td>
            <td style="text-align: center;">+2.1</td>
            <td style="text-align: center;">+2.2</td>
            <td style="text-align: center;">+1.6</td>
            <td style="text-align: center;">+0.6</td>
            <td style="text-align: center;">-1.9</td>
            <td style="text-align: center;">+2.2</td>
        </tr>
                
    </tbody>
</table>
<caption>Table2: Main results on Chart, Diagram, and Document Understanding, and Real-world Multimodal Interactions and Human Preferences benchmarks. </caption>


<table style="font-size: 13px;">
    <thead>
        <tr>
            <th rowspan="2" style="vertical-align: middle; width: 240px;">Model</th>
            <th colspan="8" style="text-align: center;">Multi-Image and Video</th>
        </tr>
        <tr>
            <th style="text-align: center;">MuirBench<br>test</th>
            <th style="text-align: center;">MEGABench<br>test</th>
            <th style="text-align: center;">EgoSchema<br>test</th>
            <th style="text-align: center;">PerceptionTest<br>test</th>
            <th style="text-align: center;">SeedBench<br>video</th>
            <th style="text-align: center;">MLVU<br>dev</th>
            <th style="text-align: center;">MVBench<br>test</th>
            <th style="text-align: center;">VideoMME<br>w/o subs</th>
        </tr>
    </thead>
    <tbody>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4o</td>
            <td style="text-align: center;">68.0</td>
            <td style="text-align: center;">54.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">64.6</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">71.9</td>
        </tr>
        <tr style="background-color: #f2f2f2;">
            <td>GPT-4v</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">60.5</td>
            <td style="text-align: center;">49.2</td>
            <td style="text-align: center;">43.5</td>
            <td style="text-align: center;">59.9</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B(Single Image)</td>
            <td style="text-align: center;">33.2</td>
            <td style="text-align: center;">-</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">60.9</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">64.8</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-72B</td>
            <td style="text-align: center;">54.8</td>
            <td style="text-align: center;">33.8</td>
            <td style="text-align: center;">62.0</td>
            <td style="text-align: center;">66.9</td>
            <td style="text-align: center;">62.1</td>
            <td style="text-align: center;">66.4</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">66.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>InternVL-2-8B</td>
            <td style="text-align: center;">59.4</td>
            <td style="text-align: center;">27.7</td>
            <td style="text-align: center;">54.2</td>
            <td style="text-align: center;">57.4</td>
            <td style="text-align: center;">54.9</td>
            <td style="text-align: center;">30.2</td>
            <td style="text-align: center;">66.4</td>
            <td style="text-align: center;">54.0</td>
        </tr>
        <tr style="background-color: #cce0ff;">
            <td>Qwen2-VL-7B</td>
            <td style="text-align: center;">41.6</td>
            <td style="text-align: center;">36.0</td>
            <td style="text-align: center;">66.7</td>
            <td style="text-align: center;">62.3</td>
            <td style="text-align: center;">55.3</td>
            <td style="text-align: center;">58.6</td>
            <td style="text-align: center;">67</td>
            <td style="text-align: center;">63.3</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B (SI)</td>
            <td style="text-align: center;">32.7</td>
            <td style="text-align: center;">22.1</td>
            <td style="text-align: center;">52.9</td>
            <td style="text-align: center;">54.9</td>
            <td style="text-align: center;">51.1</td>
            <td style="text-align: center;">60.2</td>
            <td style="text-align: center;">51.2</td>
            <td style="text-align: center;">55.0</td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>LLaVA-OV-7B</td>
            <td style="text-align: center;">41.8</td>
            <td style="text-align: center;">23.9</td>
            <td style="text-align: center;">60.1</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">56.9</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">56.7</td>
            <td style="text-align: center;">58.2</td>
        </tr>
        <tr style="border-top: 0px;">
            <td colspan="9" style="text-align: center;"></td>
        </tr>
        <tr style="background-color: #e0f7e0;">
            <td>MAmmoTH-VL-8B</td>
            <td style="text-align: center;">55.1</td>
            <td style="text-align: center;">28.2</td>
            <td style="text-align: center;">58.5</td>
            <td style="text-align: center;">59.3</td>
            <td style="text-align: center;">57.1</td>
            <td style="text-align: center;">64.7</td>
            <td style="text-align: center;">59.1</td>
            <td style="text-align: center;">58.8</td>
        </tr>
        <tr style="background-color: #E0FFFF;">
            <td>‚àÜ Over Best Open-Source</td>
            <td style="text-align: center;">+13.3</td>
            <td style="text-align: center;">+4.3</td>
            <td style="text-align: center;">-1.6</td>
            <td style="text-align: center;">+2.2</td>
            <td style="text-align: center;">+0.2</td>
            <td style="text-align: center;">+0</td>
            <td style="text-align: center;">+2.4</td>
            <td style="text-align: center;">+0.6</td>
        </tr>
                
    </tbody>
</table>
<caption>Table 3: Main results on Multi-Image and Video benchmarks.</caption>




<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Effect Of Data Scale</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To further investigate the impact of dataset size on model performance, we analyze the Stage-2 training stage for single-image. Figure 3 shows the model's overall performance across 8 benchmarks as the training dataset size increases, with each node representing 2 million data points. The results are compared against those of Qwen2-VL-8B, InternVL2-8B, and LLaVA-OneVision-8B. This trend clearly demonstrates that expanding the scale of instruction data has a significant positive effect on model performance. This observation suggests that as more diverse instruction data is introduced, the model‚Äôs ability to handle complex tasks is enhanced.
                        </p>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/mammoth_scale.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 3: Scaling effect of MAmmoTH-VL-8B on 8 representative datasets.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <!-- Added inline style for right alignment -->
                    <h2 class="title is-3">Effect Of Rewrite Model</h2>
                    
                    <div class="content has-text-justified">
                        <p>
                            To verify the impact of model size on the quality of rewritten data, we conduct experiments by training four models with 500K data: first trained on the original dataset, second on the dataset rewritten by InternVL2-Llama3-76B and Qwen2.5-72B-Instruct, third on the dataset rewritten by Qwen2-VL-7B-Instruct and Qwen2.5-7B-Instruct, fourth on the dataset rewritten by InternVL2-8B and InternLM2-5-7B. Qwen2.5-72B-Instruct, Qwen2.5-7B-Instruct and InternLM2-5-7B are only used for rewriting caption data. Qwen2-VL-72B-Instruct, Qwen2-VL-7B-Instruct and InternVL2-8B are used for data filtering.
                        </p>
                        <p>
                            The results in Figure 4 show that model trained on data rewritten based on 7B-similar-size model performs better in knowledge & reasoning and multi-interact & preference compared to the original data. However, its performance in chart & doc tasks is slightly worse, which may be due to the small model's inherent limitations in recognizing fine details in document-like images.
                        </p>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <div class="item">
                        <!-- Displaying an Image of Hybrid Instruction Tuning -->
                        <img src="static/images/ablation_rewrite_model.png" alt="Hybrid Instruction Tuning of MAmmoTH2-Plus" style="width: 100%; height: auto;" />
                        <h2 class="subtitle" style="font-size: 16px; text-align: center; color: #888888;font-weight: normal;">
                            Figure 4: Performance of data rewritten by different models on three benchmark subsets.
                        </h2>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- End image3 carousel -->


<!-- Image6 carousel -->
                                
<!-- End image6 carousel -->




<!--ending WEBINSTRUCT-->

   

<!-- 
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Overall Results</h2>
                        <div class="item">
                            Your image here -->
                            <!-- <img src="static/images/overall_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle has-text-centered">
                                Figure 2: Overall results of ü¶£MAmmoTH on the in-domain and out-of-domain datasets.
                            </h2>
                            <p>
                                Overall, we can see that MAmmoTH and MAmmoTH-Coder are able to outperform the SoTA model at different scales. In general, the performance gain for OOD datasets is more significant than IND datasets. These results show us the potential of our models as
                                a mathematical generalist. On several datasets, MAmmoTH-Coder-34B and MAmmoTH-70B are even surpassing closed-source LLMs (see more break down results below).
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->


    <!-- <section class="hero is-small">
        <div class="hero-body">
            <div class="container  is-max-desktop">
                <div class="columns is-centered has-text-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-3">Where does the gain come from?</h2>
                        <div class="item"> -->
                            <!-- Your image here -->
                            <!-- <img src="static/images/ablation_results.png" alt="MY ALT TEXT" />
                            <h2 class="subtitle">
                                Figure 3: Investigation of the influence of CoT \& PoT hybrid training on the 7B Llama-2 model. Key insights include: 1) The SoTA model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance within its domains but struggles
                                in OOD scenarios; 2) Diverse data sources in MathInstruct enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms fine-tuning on the CoT subsets; 4) Hybrid training yields the best-performing
                                model.
                            </h2>
                            <p>
                                In order to better understand what factors contribute to the great gain of ü¶£MAmmoTH over existing baselines, we set up a group of control experiments in the Figure 3. We study the following setups:
                                <ol>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - CoT):</b> This experiment aims to understand how much our curated CoT data could improve the generalization over the SoTA model WizardMath trained specifically on GSM + MATH. As can be seen,
                                        while sacrificing accuracy on GSM + MATH by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%. </li>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - PoT):</b> This experiment aims to understand the advantage of our PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall accuracy from 27% to 37.5%.
                                        This ablation reflects the importance of unlocking the program generation capabilities of our model.</li>
                                    <li>ü¶£<b>MAmmoTH (MathInstruct - Hybrid):</b> We further combine CoT and PoT as the hybrid training data to achieve the best overall performance of 45.4%. This combined gain comes from two aspects:
                                        <ul style="list-style-type: disc;">
                                            <li>
                                                The CoT subset can help maintain the generic language-based reasoning skills to handle scenarios where PoT cannot handle well, e.g., the multi-choice questions in AQuA, SAT, and MMLU.
                                            </li>
                                            <li>
                                                The PoT subset can teach the model how to utilize Python APIs to solve complex math problems with high precision, e.g., the MATH problems requiring complex computation.
                                            </li>
                                        </ul>
                                    </li>
                                </ol> -->





                                <!-- We put some case studies in Appendix \ref{sec:case_study} to demonstrate the respective advantages of PoT and CoT in solving different types of math problems. To summarize, we attribute our substantial gain to: 1) diverse data sources covering different math fields and complexity levels and 2) a hybrid of CoT \& PoT instruction tuning strategy.  -->

                            <!-- </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section> -->





<footer class="footer">
    <!-- <div class="container"> -->
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is website adapted from <a href="https://tiger-ai-lab.github.io/MAmmoTH/">MAmmoTH</a> and <a href="https://mmmu.github.io/">MMMU</a>, licensed under a <a rel="license"
                                                  href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    <!-- </div> -->
  
  </footer>

<!--table4-->
</html>
